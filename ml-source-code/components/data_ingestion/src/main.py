"""
Data Ingestion Component - MLOps Daily Pipeline

M·ª•c ƒë√≠ch:
    Thu th·∫≠p d·ªØ li·ªáu raw t·ª´ nhi·ªÅu ngu·ªìn (APIs, databases, files) v√† l∆∞u v√†o S3 data lake.
    ƒê√¢y l√† b∆∞·ªõc ƒë·∫ßu ti√™n trong ML pipeline, ch·∫°y h√†ng ng√†y ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi nh·∫•t.

Workflow:
    1. K·∫øt n·ªëi v·ªõi data sources (APIs, databases)
    2. Extract raw data t·ª´ c√°c ngu·ªìn
    3. Validate data format v√† schema c∆° b·∫£n
    4. Upload raw data l√™n S3 bucket t·∫°i prefix raw/{date}/
    5. Ghi log metadata v·ªÅ data ingestion (s·ªë l∆∞·ª£ng records, timestamp)

Input:
    - Data sources: External APIs, databases, file systems
    - Config: S3 bucket name, data source credentials

Output:
    - Raw data files trong S3: s3://{bucket}/raw/{date}/{source}_{timestamp}.parquet
    - Metadata log: s3://{bucket}/raw/{date}/metadata.json

Environment Variables:
    - S3_DATA_LAKE_BUCKET: S3 bucket name for data lake
    - DATA_SOURCE_CONFIG: JSON config for data sources
    - LOG_LEVEL: Logging level (INFO, DEBUG, ERROR)

Example:
    python -m src.main
    
    Output:
    - s3://ml-fashion-data-lake/raw/2025-01-15/api_fashion_20250115_020000.parquet
    - s3://ml-fashion-data-lake/raw/2025-01-15/db_users_20250115_020000.parquet
    - s3://ml-fashion-data-lake/raw/2025-01-15/metadata.json

MLOps Integration:
    - Ch·∫°y t·ª± ƒë·ªông h√†ng ng√†y qua Argo CronWorkflow
    - Trigger data processing step sau khi ingestion ho√†n th√†nh
    - Monitor data quality v√† alert n·∫øu c√≥ anomalies
"""
import os
import json
import logging
from datetime import datetime
from typing import Dict, Any

# Setup logging
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def ingest_data_from_source(source_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    M√¥ ph·ªèng vi·ªác ingest data t·ª´ m·ªôt source.
    
    Args:
        source_name: T√™n c·ªßa data source (e.g., 'api_fashion', 'db_users')
        config: Configuration cho data source
        
    Returns:
        Dict ch·ª©a metadata v·ªÅ data ƒë√£ ingest
    """
    logger.info(f"Starting data ingestion from source: {source_name}")
    
    # M√¥ ph·ªèng: Gi·∫£ s·ª≠ ƒë√£ fetch ƒë∆∞·ª£c data
    record_count = 1000  # Simulated record count
    timestamp = datetime.utcnow().isoformat()
    
    # M√¥ ph·ªèng upload l√™n S3
    bucket = os.getenv("S3_DATA_LAKE_BUCKET", "ml-fashion-data-lake")
    date_prefix = datetime.utcnow().strftime("%Y-%m-%d")
    s3_key = f"raw/{date_prefix}/{source_name}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.parquet"
    
    logger.info(f"Simulated upload to s3://{bucket}/{s3_key}")
    logger.info(f"Records ingested: {record_count}")
    
    return {
        "source": source_name,
        "record_count": record_count,
        "s3_bucket": bucket,
        "s3_key": s3_key,
        "timestamp": timestamp,
        "status": "success"
    }


def main():
    """Main entry point for data ingestion component."""
    logger.info("=" * 60)
    logger.info("Data Ingestion Component - Starting")
    logger.info("=" * 60)
    
    # Get configuration
    bucket = os.getenv("S3_DATA_LAKE_BUCKET", "ml-fashion-data-lake")
    component_name = os.getenv("COMPONENT_NAME", "data_ingestion")
    
    logger.info(f"Component: {component_name}")
    logger.info(f"S3 Bucket: {bucket}")
    
    # M√¥ ph·ªèng ingest t·ª´ nhi·ªÅu sources
    sources = [
        {"name": "api_fashion", "type": "rest_api"},
        {"name": "db_users", "type": "database"},
        {"name": "file_products", "type": "file_system"}
    ]
    
    results = []
    for source in sources:
        try:
            result = ingest_data_from_source(source["name"], source)
            results.append(result)
            logger.info(f"‚úÖ Successfully ingested from {source['name']}")
        except Exception as e:
            logger.error(f"‚ùå Failed to ingest from {source['name']}: {str(e)}")
            results.append({
                "source": source["name"],
                "status": "failed",
                "error": str(e)
            })
    
    # M√¥ ph·ªèng l∆∞u metadata
    metadata = {
        "ingestion_date": datetime.utcnow().isoformat(),
        "component": component_name,
        "sources": results,
        "total_records": sum(r.get("record_count", 0) for r in results if r.get("status") == "success")
    }
    
    logger.info(f"üìä Ingestion Summary:")
    logger.info(f"   - Total sources: {len(sources)}")
    logger.info(f"   - Successful: {sum(1 for r in results if r.get('status') == 'success')}")
    logger.info(f"   - Total records: {metadata['total_records']}")
    
    logger.info("=" * 60)
    logger.info("Data Ingestion Component - Completed")
    logger.info("=" * 60)
    
    return metadata


if __name__ == "__main__":
    main()